{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from create_captions import CreateCaption\n",
    "from pyannote.audio import Pipeline\n",
    "from utils import download_youtube_video\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from utils import SpeakerDiarization\n",
    "import whisper\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please provide Video details and tokens below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Video link to summarize\n",
    "VIDEO_LINK = \"https://www.youtube.com/watch?v=O-IitVoKASo\"\n",
    "# Authentication token from hugging face to load VLM\n",
    "PYAN_AUTH_TOKEN = \"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\"\n",
    "# VLM model to be used 'Salesforce/blip2-flan-t5-xxl' or '\"liuhaotian/llava-v1.5-7b'\n",
    "VLM_MODEL = \"Salesforce/blip2-flan-t5-xxl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Speaker diarization module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"\n",
    "    Load Whisper and pyannote audio models for processing transcript.\n",
    "    \"\"\"\n",
    "\n",
    "    model = whisper.load_model(\"base\")\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization\",\n",
    "        use_auth_token=PYAN_AUTH_TOKEN,\n",
    "    ).to(device)\n",
    "    sd = SpeakerDiarization(model, pipeline)\n",
    "    return sd\n",
    "\n",
    "\n",
    "sd = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_path = download_youtube_video(VIDEO_LINK, \"video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GMFLow\n",
      "Loading VLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020d782c62de4837aa1efe845e3eb664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Models\n"
     ]
    }
   ],
   "source": [
    "# Loading VLM will take some time for the first time run to download weights\n",
    "\n",
    "cc = CreateCaption(\n",
    "    device,\n",
    "    gm_loc=\"gmflow_sintel-0c07dcb3.pth\",\n",
    "    model_type=VLM_MODEL,\n",
    "    frames_to_skip=25,  # number of frames to skip before analysing default is 5 but increase to process faster\n",
    "    batch_size=64,  # Decrease batch size if you have low gpu VRAM\n",
    ")\n",
    "get_captions = lambda video_loc: cc.captions(video_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------------------------------------->] \n",
      "  \n",
      " Total frames read 1120/5668 \n",
      " time = 186.85333333333332\n",
      " good frames =45\n",
      " 1/1 videos \n",
      "time 0.0s \n",
      "time_spent 73.37938046455383s\n"
     ]
    }
   ],
   "source": [
    "# get captions for important frames\n",
    "\n",
    "caps = get_captions(r\"videos/video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete CreateCaptions object to free VRAM if needed\n",
    "del cc\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get access to LLama 2 - 13B\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\", load_in_4bit=True, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load PEFT adapter\n",
    "peft_model_id = \"Basha738/outputs\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "adapter_model = PeftModel.from_pretrained(original_model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "\n",
    "def gen(text):\n",
    "    toks = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    set_seed(32)\n",
    "    adapter_model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = adapter_model.generate(\n",
    "            **toks,\n",
    "            max_new_tokens=350,\n",
    "            top_k=5,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    return tokenizer.decode(\n",
    "        out[0][len(toks[\"input_ids\"][0]) :], skip_special_tokens=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"### Instruction\n",
    "Given the transcript of a video with identified speakers as \"Speaker 1: ...\" and \"Speaker 2: ...\", along with captions highlighting key moments in the video, your objective is to create a concise summary of the video's content. Your analysis will consider both the transcript and the captions, without explicitly referring to them.\n",
    "\n",
    "Please pay attention to the primary speaker. If speaker names are mentioned in the captions, please infer them. However, if names are not clear, proceed to generate the summary without assuming them as \"Speaker 1\" or \"Speaker 2\".\n",
    "\n",
    "Based on the tone of the video, change the tone in the square brackets to one of the following:\n",
    "['Informative', 'Neutral', 'Relaxation', 'Encouraging', 'Enthusiastic', 'Frustration', 'Cautious', 'Sarcasm', 'Optimistic']\n",
    "\n",
    "Based on the category of the video, change the category in the square brackets to one of the following:\n",
    "['Finance', 'Investment', 'Business', 'Entrepreneurship', 'Branding', 'Macroeconomics', 'Real Estate ', 'Economical Situation', 'Entertainment Market', 'Business Ethics', 'Real Estate']\n",
    "\n",
    "# Transcript:\n",
    "{transcript}\n",
    "\n",
    "# Captions:\n",
    "{captions}\n",
    "\n",
    "### Response:\n",
    "Summary:\n",
    "```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = sd.get_script(\"videos/video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen(template.format(transcript=script, captions={x[0] for x in caps})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter your youtube link below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_LINK = \"<ENTER VIDEO LINK TO PROCESS>\"\n",
    "video_path = download_youtube_video(VIDEO_LINK, \"video_2.mp4\")\n",
    "caps = get_captions(r\"videos/video_2.mp4\")\n",
    "script = sd.get_script(\"videos/video_2.mp4\")\n",
    "print(gen(template.format(transcript=script, captions={x[0] for x in caps})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basha",
   "language": "python",
   "name": "basha"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
